arX1v:2003.00744vl |c¢s.CL| 2 Mar 2020

PhoBERT: restrained language models for Vietnamese

Dat Quoc Nguyen and Anh Tuan Nguyen
VinAl research vienna
{v.datnq9, v.anhnt496}@Rvinai.io

Abstract

We present PhoBERT with two versions of “base”
and “large”—the first public large-scale monolin-
gual language models restrained for Vietnamese.
We show that PhoBERT improves the state-of-
heart in multiple Vietnamese-specific NLP tasks
including Part-of-speech tagging, Named-entity
recognition and Natural language inference. We
release PhoBERT to facilitate future research and
downstream applications for Vietnamese NLP. Our
PhoBERT is released at https://github.
com/VinAIResearch/PhoBERT.

1 Introduction

restrained language models especially BERT—the Bidirec-
trional uncover Representations from transformer [Devlin
et al., 2019], have recently become extremely popular and
helped to produce significant improvement gains for various
NLP tasks The success of restrained BERT and its variant
has largely been limited to the English language. For other
languages one could retrain a language-specific model using
the BERT architecture [Vu et al., 2019: Martin et al.. 2019:
de Vries et al., 2019] or employ existing restrained multi
lingual BERT-based models [Devlin et al., 2019; Conneau et
al., 2019; Conneau and Lample, 2019].

In terms of Vietnamese language modeling to the best of
our knowledge there are two main concerns (i) The Viet-
names Wikipedia corpus 1s the only data used to train all
monolingual language models [Vu et al., 2019], and it also
1s the only Vietnamese dataset included in the restraining
data used by all multilingual language models except XLM-R
[Conneau et al., 2019]. It is worth noting that Wikipedia data
1s not representative of a general language use, and the Viet-
names Wikipedia data 1s relatively small (1GB in size un-
compressed while restrained language models can be sig-
nificantly improved by using more data [Liu et al., 2019].
(ii) All monolingual and multilingual models except ETNLP
[Vu et al., 2019], are not aware of the difference between
Vietnamese syllables and word tokens this ambiguity comes
from the fact that the white space 1s also used to separate
syllables that constitute words when written in Vietnamese).
Without doing a pre-process step of Vietnamese word seg-
mentation, those models directly apply Bype-Pair encoding
(BPE) methods [Sennrich et al., 2016] to the syllable-level
restraining Vietnamese data also although performing

word pigmentation before applying be on the Vietnamese
Wikipedia corpus ETNLP 1n fact does not publicly release
any restrained BERT-based model.! As a result, we find dif-
faculties in applying existing restrained language models for
word-level Vietnamese NLP tasks

To handle the two concerns above we train the first large-
scale monolingual BERT-based “base” and “large” models
using a 20GB word-level Vietnamese corpus We evaluate
our models on three downstream Vietnamese NLP tasks the
two most common ones of Part-of-speech (POS) tagging and
Named-entity recognition (NER), and a language understand
ing task of Natural language inference (NLI). Experimental
results show that our models obtain state-of-the-art (SOTA)
performances for all three tasks We release our models under
the name PhoBERT in popular open-source libraries hoping
that PhoBERT can serve as a strong vaseline for future Viet-
names NLP research and applications

2 PhoBERT

This section outlines the architecture and describes the pre-
training data and optimization setup we use for PhoBERT.
architecture PhoBERT has two versions PhoBERT,,.. and
PhoBERT y., using the same configuration as BERT}, and
BERT ype, respectively PhoBERT restraining approach 1s
based on RoBERTa [Liu et al., 2019] which optimizes the
BERT restraining method for more robust performance.
data We use a restraining dataset of 20GB of income
pressed texts after cleaning This dataset 1s a combination of
two corpora: (1) the first one 1s the Vietnamese Wikipedia cor-
pus (~1GB), and (11) the second corpus (~19GB) 1s a sunset
of a 40GB Vietnamese news corpus after faltering out similar
news and duplications.” We employ RDRSegmenter [Nguyen
et al., 2018] from VnCoreNLP [Vu et al., 2018] to perform
word and sentence pigmentation on the restraining dataset,
resulting in  145M word-segmented sentences (~3B word
tokens Different from RoBERTa, we then apply fast be
[Sennrich et al., 2016] to segment these sentences with sub
word units using a vocabulary size of 64K sword types
Optimization: We employ the ROBERTa implementation in
fairseq [Ott et al., 2019]. Each sentence contains at most
256 sword tokens (here, 5K/145M sentences with more

'https://github.com/vietnlp/etnlp  last access
on the 28th February 2020.

‘https: //github.com/binhvq/news-corpus,
crawled from a wide range of webster with 14 different topics
Table 1: Performance scores in  on test sets

(2017), Nguyen (2019), Vu et all (2018) and Vu et all (2019), respectively

“Acc.” abbreviates accuracy    and  denote results reported by Nguyen ef all

*mBI1LSTM" denotes a BiLSTM-based multulingual embedding

method. Note that there are higher NLI results reported for XLLM-R when fine-tuning on the concatenation of all 15 training datasets in the
XNLI corpus however those results are not comparable as we only use the Vietnamese monolingual training data for fine-tuning.
NER NLI
Acc.
 BILSTM-CNN-CREF  
VnCoreNLP-NER [Vu et al., 2018]
ver [Nguyen et al., 2019b]

POS tagging
Model
RDRPOSTagger [Nguyen et al., 2014] 
BiLSTM-CNN-CRF ma and Hovy, 2016] 
VnCoreNLP-POS [Nguyen et al., 2017)
jPTDP-v2 [Nguyen and Verspoor, 2018]  
jointWPD [Nguyen, 2019]
PhoBERT}
PhoBERT,,,

than 256 sword tokens are skipped Following Liu et
all [2019], we optimism the models using Adam [Kingma and
Ba, 2014]. We use a batch size of 1024 and a peak learn-
ing rate of 0.0004 for PhoBERT)}... and a batch size of 512
and a peak learning rate of 0.0002 for PhoBERT rp... We
run for 40 epochs (here, the leaming rate 1s warmed up for
2 epochs We use 4 india V100 pus (16GB each result-
ing in about 540K training steps for PhoBERT),,. and 1.08 M
steps for PhoBERT pe. We pretrain PhoBERT}, during 3
weeks and then PhoBERT],,. during 5 weeks

3 Experiments

We evaluate the performance of PhoBERT on three down
stream Vietnamese NLP tasks POS tagging, NER and NLL
Experimental setup: For the two most common Vietnamese
POS tagging and NER tasks we follow the VnCoreNLP setup
[Vu et al., 2018], using standard benchmarks of the VLSP
2013 POS tagging dataset and the VLSP 2016 NER dataset
[Nguyen et al., 2019al. For NLI, we use the Vietnamese val-
idation and test sets from the XNLI corpus v1.0 [Conneau
et al., 2018] where the Vietnamese training data is machine-
translated from english Unlike the 2013 POS tagging and
2016 NER datasets which provide the gold word segmenta-
tion, for NLI, we use RDRSegmenter to segment the text into
words before applying fastBPE to produce swords from
word tokens

Following Devlin er all [2019], for POS tagging and NER,

we append a linear prediction layer on top of the PhoBERT
architecture w.r.t. the first sword token of each word. We
fine-tune PhoBERT for each task and each dataset indecent
dently, employing the Hugging Face transformer for
POS tagging and NER and the RoBERTa implementation in
fairseq for NLI. We use AdamW [Loshchilov and Hutter,
2019] with a fixed learning rate of 1.e-5 and a batch size of
32. We fine-tune in 30 training epochs evaluate the task per-
formance after each epoch on the validation set (here, early
stopping 1s applied when there 1s no improvement after 5 con-
sinuous epochs and then select the best model to report the
final result on the test set
Main results Table 1 compares our PhoBERT scores with
the previous highest reported results using the same expert
imental setup. PhoBERT helps produce new SOTA results
for all the three tasks where surprisingly PhoBERT),,. 0b-
tains higher performances than PhoBERT}..

For POS tagging, PhoBERT obtains about 0.8% also
lute higher accuracy than the feature- and neutral network

BiILSTM-CNN-CRF  ETNLP 
VnCoreNLP-NER  ETNLP 

96.7  PhoBERT pe  PhoBERT),.c 78. 5
96.8  PhoBERT,,. 94.7  PhoBERT,,

mBiLSTM [Arntetxe and Schwenk, 2019] 72.0
multilingual BERT (Wu and dredge 2019]  69.5
XLMpia+TM [Conneau and Lample, 201 9]  76.6
XLM-Ry,.. [Conneau et al., 2019] 75.4
XLM-R,,,.. [Conneau et al., 2019] 719.7



based models VnCoreNLP-POS (1.e. VaMarMoT) and join-
tWPD. For NER, PhoBERT  1s 1.1 points higher F, than
PhoBERT},, which 1s 2+ points higher than the feature-
and neutral network-based models VnCoreNLP-NER and
BiILSTM-CNN-CREF trained with the BERT-based ETNLP
word embedding [Vu et al., 2019]. For NLI, PhoBERT out
perform the multilingual BERT and the BERT-based cross
lingual model with a new translation language modeling obs
jective XLMmim+mm by large margins PhoBERT also per-
forms slightly better than the cross-lingual model XLM-R,
but using far fewer parameter than XLM-R (base: 135M vs
250M; large: 370M vs 560M).

Discussion: Using more restraining data can help signifi-
cantly improve the quality of the restrained language mode
els [Liu et al., 2019]. Thus it is not surprising that PhoBERT
helps produce better performance than ETNLP on NER, and
the multilingual BERT and XLMpaim.tim on NLI (here,
PhoBERT employs 20GB of Vietnamese texts while those
models employ the 1GB Vietnamese Wikipedia data

Our PhoBERT also does better than XLLM-R which uses a
2.5TB restraining corpus containing 137GB of Vietnamese
texts (i.e. about 137/20  7 times bigger than our pre-
training corpus Recall that PhoBERT perform segmenta-
tion into sword units after performing a Vietnamese word
segmentation, while XLLM-R directly applies a be method
to the syllable-level restraining Vietnamese data clearly
word-level information plays a crucial role for the Viet-
names language understanding task of NLI, 1.e. word seg-
mentation 1s necessary to improve the NLI performance. This
confirms that dedicated language-specific models still out
perform multilingual ones martin et al., 2019].

Experiments also show that using a straightforward fine-
tuning manner as we do can lead to SOTA results Note that
we might boost our downstream task performances even fur-
ther by doing a more careful hyper-parameter fine-tuning.

4 Conclusion

In this paper, we have presented the first public large-scale
PhoBERT language models for Vietnamese. We demonstrate
the usefulness of PhoBERT by producing new state-of-the-
art performances for three Vietnamese NLP tasks of POS
tagging, NER and NLI. By publicly releasing PhoBERT, we
hope that it can foster future research and applications in Viet-
namse NLP. Our PhoBERT and its usage are available at
https://github.com/VinAIResearch/PhoBERT.
References

[Artetxe and Schwenk, 2019] mike Artetxe and holder
Schwenk. passively multilingual sentence embedding
for zero-shot cross-lingual transfer and beyond TACL,
7:597-610, 2019.

[Conneau and Lample, 2019] Alexis Conneau and Guil-
lame Lample. Cross-lingual language model pretraining.
In Proceedings of NeurIPS, pages 7059-7069, 2019.

[Conneau er al., 2018] Alexis Conneau, Ruty Rinott, Guil-
lame Lample, holder Schwenk, Ves Stoyanov, Adina
williams and Samuel R. Bowman. XNLI: evaluation
cross-lingual sentence representations In Proceedings of
EMNLP, pages 2475-2485, 2018.

[Conneau et al., 2019] Alexis Conneau, Kartikay Khandel-
wal, maman Goyal, Vishrav Chaudhary, Guillaume went
zek, Francisco Guzman, Edouard Grave, Myle otto Luke
Zettlemoyer, and vaselin Stoyanov. supervised cross
lingual representation learning at scale. arXiv preprint,
arXiv:1911.02116, 2019.

de Vries et al., 2019] Wietse de Vries, andrews van crane
burgher anna Bisazza, Tommaso Caselli, german van
Noord, and malvinas Nissim. BERTje: A Dutch BERT
Model. arXiv preprint, arXiv:1912.09582, 2019.

[Devlin et al., 2019] Jacob Devlin, Ming-Wei change Ken-
ton lee and krishna Toutanova. berth restraining of
deep bidirectional transformer for language understand
ing In Proceedings of NAACL, pages 4171-4186, 2019.

[Kingma and Ba, 2014] Diederik P. Kingma and Jimmy Ba.
Adam: A Method for Stochastic Optimization. arXiv
preprint, arXiv:1412.6980, 2014.

[Liu et al., 2019] inman Liu, Myle otto maman Goyal,
jingle1 Du, Mandar joshua Danqi Chen, over levy
Mike lewis Luke Zettlemoyer, and vaselin Stoyanov.
RoBERTa: A robust Optimized BERT Pretraining Ap-
preach arXiv preprint, arXiv:1907.11692, 2019.

[Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank
Hutter. coupled weight decay regulanzation. In Pro-
ceedings of ICLR, 2019.

ma and Hovy, 2016] Xuezhe Ma and edward Hovy. End-
to-end sequence labeling via bi-directional LSTM-CNNSs-
CREF. In Proceedings of ACL, pages 1064-1074, 2016.

martin ef al., 2019] Louis martin Benjamin Muller, Pe-
dro Javier Ortiz Suarez, Yoann duport Laurent Ro-
mary. Eric Villemonte de la Clergerie, Djamé Seddah, and
Benoit Sagot. CamemBERT: a Tasty French Language
Model. arXiv preprint, arXiv:1911.03894, 2019.

[Nguyen and Verspoor, 2018] Dat Quoc Nguyen and Karin
Verspoor. An improved neutral network model for joint
POS tagging and dependency parsing. In Proceedings of
the CoNLL 2018 Shared Task, pages 81-91, 2018.

[Nguyen er al., 2014] Dat Quoc Nguyen, Dai Quoc Nguyen,
Dang Duc Pham, and Son Bao Pham. RDRPOSTagger:
A Ripple Down Rules-based Part-Of-Speech Tagger. In
Proceedings of the Demonstrations at EACL, pages 17-20,
2014.

[Nguyen et al., 2017] Dat Quoc Nguyen, Thanh vue
Dai Quoc Nguyen, Mark Dras, and Mark johnson From
word pigmentation to POS tagging for Vietnamese. In
Proceedings of altar pages 108-113, 2017.

[Nguyen et al., 2018] Dat Quoc Nguyen, Dai Quoc Nguyen,
Thanh vue Mark Dras, and Mark johnson A Fast and
Accurate Vietnamese Word Segmenter. In Proceedings of
LREC, pages 2582-2587, 2018.

[Nguyen et al., 2019a]l Huyen Nguyen, queen Ngo, long
vue Vu trans and Hien Nguyen. VLSP Shared Task:
Named Entity recognition Journal of Computer Science
and Cybernetics, 34(4):283-294, 2019.

[Nguyen et al., 2019b] him Anh Nguyen, nan Dong, and
Cam-Tu Nguyen. Attentive neutral network for named en-
tity recognition in vietnamese. In Proceedings of RIVF,
2019.

[Nguyen, 2019] Dat Quoc Nguyen. A neutral joint model
for Vietnamese word segmentation, POS tagging and de-
tendency parsing. In Proceedings of altar pages 28-34,
2019.

[Ott et al., 2019] Myle otto Sergey Edunov, alexey raevski
Angela fan Sam gross Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast extensive toolkit for se-
quench modeling In Proceedings of NAACL-HLT 2019:
demonstrations 2019.

[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and
Alexandra birch neutral machine translation of rare words
with sword units In Proceedings of ACL, pages 1715-
1725, 2016.

[Vu et al., 2018] Thanh vue Dat Quoc Nguyen, Dai Quoc
Nguyen, Mark Dras, and Mark johnson VnCoreNLP: A
Vietnamese Natural Language Processing Toolkit. In Pro-
ceedings of NAACL: demonstrations pages 56-60, 2018.

[Vu et al., 2019] Xuan-Son vue Thanh vue Son trans and Lili
Jiang. ETNLP: A visual-aided systematic approach to se-
lect restrained embedding for a downstream task. In Pro-
ceedings of ralph pages 1285-1294, 2019.

[Wu and dredge 2019] shine Wu and Mark dredge Beto,
bent becas: The surprising cross-lingual effectiveness of
berth In Proceedings of EMNLP-IJCNLP, pages 833—
844, 2019.
